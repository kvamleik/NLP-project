{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kvamleik/NLP-project/blob/main/Data_preprocessing_EasyJet_ORIGINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eTfyybLz14S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import spacy \n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaDZ-wUMz14b"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/Users/nikolajfrandsen/Desktop/Master Thesis Data/Twitter/EasyJet/EasyJet_Twitter_CLEAN.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD434NUtz14e",
        "outputId": "639291ee-39cb-42ab-bca4-b93624e30305"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Country</th>\n",
              "      <th>Hit Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01-Aug-2019 01:00AM</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>=@bell_allie @SouthendAirport @easyJet Probabl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01-Aug-2019 01:01AM</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>@RGrosjean @easyJet Go Pietro Go!!!____</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01-Aug-2019 01:01PM</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>@easyJet why does your submission form on your...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01-Aug-2019 01:01PM</td>\n",
              "      <td>Italy</td>\n",
              "      <td>@Gatwick_Airport Hi, my flight to Pisa, with E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01-Aug-2019 01:02AM</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>=@brothers_beyond @easyJet Hi @brothers_beyond...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Date         Country  \\\n",
              "0  01-Aug-2019 01:00AM  United Kingdom   \n",
              "1  01-Aug-2019 01:01AM         Unknown   \n",
              "2  01-Aug-2019 01:01PM         Unknown   \n",
              "3  01-Aug-2019 01:01PM           Italy   \n",
              "4  01-Aug-2019 01:02AM         Unknown   \n",
              "\n",
              "                                        Hit Sentence  \n",
              "0  =@bell_allie @SouthendAirport @easyJet Probabl...  \n",
              "1            @RGrosjean @easyJet Go Pietro Go!!!____  \n",
              "2  @easyJet why does your submission form on your...  \n",
              "3  @Gatwick_Airport Hi, my flight to Pisa, with E...  \n",
              "4  =@brothers_beyond @easyJet Hi @brothers_beyond...  "
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjR0vtAbz14j"
      },
      "source": [
        "# Lower casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGcV43yWz14k",
        "outputId": "9057301c-e349-4702-c315-665db52761c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    =@bell_allie @southendairport @easyjet probabl...\n",
              "1              @rgrosjean @easyjet go pietro go!!!____\n",
              "2    @easyjet why does your submission form on your...\n",
              "3    @gatwick_airport hi, my flight to pisa, with e...\n",
              "4    =@brothers_beyond @easyjet hi @brothers_beyond...\n",
              "Name: text_lower, dtype: object"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text_lower'] = df['Hit Sentence'].str.lower()\n",
        "df['text_lower'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp29rLsuz14n"
      },
      "source": [
        "# Removal of punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CApLLMU0z14p",
        "outputId": "1a26960c-2af6-46e2-fe2a-33ba5ded0fd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    bell_allie southendairport easyjet probably to...\n",
              "1                   rgrosjean easyjet go pietro go____\n",
              "2    easyjet why does your submission form on your ...\n",
              "3    gatwick_airport hi my flight to pisa with easy...\n",
              "4    brothers_beyond easyjet hi brothers_beyond and...\n",
              "Name: text_punct, dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text_punct'] = df['text_lower'].str.replace('[^\\w\\s]','')\n",
        "df['text_punct'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKz6Uk_oz14t"
      },
      "source": [
        "# Stop-word removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMR8ljd8z14v"
      },
      "outputs": [],
      "source": [
        "# Importing stopwords from nltk library (Sometimes it is necessart to use another import - system will show so)\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WwEfZS_z141"
      },
      "outputs": [],
      "source": [
        "# Function to remove the stopwords\n",
        "def stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prv9JIi9z146",
        "outputId": "07bc8f49-7924-4ef3-ab76-db93997b99d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    bell_allie southendairport easyjet probably fl...\n",
              "1                   rgrosjean easyjet go pietro go____\n",
              "2    easyjet submission form website work sent emai...\n",
              "3    gatwick_airport hi flight pisa easyjet delayed...\n",
              "4    brothers_beyond easyjet hi brothers_beyond eas...\n",
              "Name: text_stop, dtype: object"
            ]
          },
          "execution_count": 11,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Applying the stopwords to 'text_punct' and store into 'text_stop'\n",
        "df['text_stop'] = df['text_punct'].apply(stopwords)\n",
        "df['text_stop'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuLniHHVz149"
      },
      "source": [
        "# Common word removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcLAx1Ziz14_",
        "outputId": "de67ec57-cfd0-4f62-b40b-94bb8a1ca982"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('easyjet', 128450),\n",
              " ('mattiasharris', 50212),\n",
              " ('rt', 49720),\n",
              " ('flight', 43365),\n",
              " ('ryanair', 38283),\n",
              " ('easyjet_press', 37440),\n",
              " ('easa', 36960),\n",
              " ('iata', 36955),\n",
              " ('geneveaeroport', 36864),\n",
              " ('seats', 35999)]"
            ]
          },
          "execution_count": 12,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Checking the first 10 most frequent words\n",
        "from collections import Counter\n",
        "cnt = Counter()\n",
        "for text in df[\"text_stop\"].values:\n",
        "    for word in text.split():\n",
        "        cnt[word] += 1\n",
        "        \n",
        "cnt.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcFHlbIZz15C",
        "outputId": "07d79b33-7200-44ca-881b-d2bfd3ad3461"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    bell_allie southendairport probably fly schedu...\n",
              "1                           rgrosjean go pietro go____\n",
              "2    submission form website work sent email receiv...\n",
              "3    gatwick_airport hi pisa delayed 2 hours kind s...\n",
              "4    brothers_beyond hi brothers_beyond ask ref lut...\n",
              "Name: text_common, dtype: object"
            ]
          },
          "execution_count": 13,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Removing the frequent words\n",
        "\n",
        "freq = set([w for (w, wc) in cnt.most_common(10)])\n",
        "\n",
        "# Function to remove the frequent words\n",
        "\n",
        "def freqwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in freq])\n",
        "\n",
        "# Passing the function to freqwords\n",
        "\n",
        "df[\"text_common\"] = df[\"text_stop\"].apply(freqwords)\n",
        "df[\"text_common\"].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5W0sTghz15F"
      },
      "source": [
        "# Rare word removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFUbS5caz15G",
        "outputId": "f0052f29-af1a-4e8e-cfcc-7f3d7b94c0c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    bell_allie southendairport probably fly schedu...\n",
              "1                           rgrosjean go pietro go____\n",
              "2    submission form website work sent email receiv...\n",
              "3    gatwick_airport hi pisa delayed 2 hours kind s...\n",
              "4    brothers_beyond hi brothers_beyond ask ref lut...\n",
              "Name: text_rare, dtype: object"
            ]
          },
          "execution_count": 14,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Removal of 10 rare words in store into new coloumn called 'text_rare'\n",
        "freq = pd.Series(' '.join(df['text_common']).split()).value_counts()[-10:] #10 rare words\n",
        "freq = list(freq.index)\n",
        "df['text_rare'] = df['text_common'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "\n",
        "df['text_rare'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e20bzq3Iz15J"
      },
      "source": [
        "# Spelling correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B07IdnYNz15K",
        "outputId": "9898da2b-d3b1-4d3c-a656-53eec2ea801b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "sRflaY_2z15O",
        "outputId": "50083a1b-c452-4f26-9ced-8b750dad59ad"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c9943bb9820d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Spell check using text blob for the first 5 records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_rare'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# Spell check using text blob for the first 5 records\n",
        "from textblob import TextBlob\n",
        "df['text_rare'][:5].apply(lambda x: str(TextBlob(x).correct()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x02YuKsz15R"
      },
      "source": [
        "# Emoji removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_redicWjz15T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "f6980a4e-ccf6-40ad-d404-f3787aecaff4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4d91dcb96278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#passing the emoji function to 'text_rare'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_rare'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_rare'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_emoji\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# Function to remove emoji.\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "emoji = (\"Hi, I am Emoji  ðŸ˜œ\")\n",
        "\n",
        "#passing the emoji function to 'text_rare'\n",
        "df['text_rare'] = df['text_rare'].apply(remove_emoji)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyrL381Pz15W"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkjTpDNOz15Z"
      },
      "source": [
        "# Emoticon removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W8Bi-ZEz15a",
        "outputId": "014e2989-8260-4e3d-ac00-f8d7d57857c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emot in /Users/nikolajfrandsen/anaconda3/lib/python3.7/site-packages (2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install emot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXI_E1-7z15e"
      },
      "outputs": [],
      "source": [
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "\n",
        "# Function for removing emoticons\n",
        "def remove_emoticons(text):\n",
        "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
        "    return emoticon_pattern.sub(r'', text)\n",
        "remove_emoticons(\"Hello :-)\")\n",
        "\n",
        "# applying remove_emoticons to 'text_rare'\n",
        "df['text_rare'] = df['text_rare'].apply(remove_emoticons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skW6SKo0z15h",
        "outputId": "ecc9f458-c90f-4750-b3b0-6f1d2b288cd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    bell_allie southendairport probably fly schedu...\n",
              "1                           rgrosjean go pietro go____\n",
              "2    submission form website work sent email receiv...\n",
              "3    gatwick_airport hi pisa delayed 2 hours kind s...\n",
              "4    brothers_beyond hi brothers_beyond ask ref lut...\n",
              "Name: text_rare, dtype: object"
            ]
          },
          "execution_count": 21,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text_rare'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLfA8E-bz15l"
      },
      "source": [
        "# Converting emoji and emoticons to words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8U8joHQz15m"
      },
      "outputs": [],
      "source": [
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "\n",
        "# Converting emojis to words\n",
        "def convert_emojis(text):\n",
        "    for emot in UNICODE_EMO:\n",
        "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
        "        return text\n",
        "    \n",
        "# Converting emoticons to words    \n",
        "def convert_emoticons(text):\n",
        "    for emot in EMOTICONS:\n",
        "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
        "        return text\n",
        "    \n",
        "# Example\n",
        "text = \"Hello :-) :-)\"\n",
        "convert_emoticons(text)\n",
        "text1 = \"Hilarious ðŸ˜‚\"\n",
        "convert_emojis(text1)\n",
        "\n",
        "# Passing both functions to 'text_rare'\n",
        "df['text_rare'] = df['text_rare'].apply(convert_emoticons)\n",
        "df['text_rare'] = df['text_rare'].apply(convert_emojis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Que2NQQvz15p"
      },
      "source": [
        "# Removal of URL's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFTNSx6Hz15q",
        "outputId": "da5e4593-a7f4-44b7-cc1e-1285140f2e4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    None\n",
              "1    None\n",
              "2    None\n",
              "3    None\n",
              "4    None\n",
              "Name: text_urls, dtype: object"
            ]
          },
          "execution_count": 23,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function for url's\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    \n",
        "# Examples\n",
        "text = \"this is my website, https://www.abc.com\"\n",
        "remove_urls(text)\n",
        "\n",
        "# Passing the function to 'text_rare'\n",
        "df['text_urls'] = df['text_rare'].apply(remove_urls)\n",
        "df['text_urls'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P7eqYBez15t",
        "outputId": "fe0625d9-e069-45f2-8c5a-bf6fa5d72210"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Country</th>\n",
              "      <th>Hit Sentence</th>\n",
              "      <th>text_lower</th>\n",
              "      <th>text_punct</th>\n",
              "      <th>text_stop</th>\n",
              "      <th>text_common</th>\n",
              "      <th>text_rare</th>\n",
              "      <th>text_urls</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01-Aug-2019 01:00AM</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>=@bell_allie @SouthendAirport @easyJet Probabl...</td>\n",
              "      <td>=@bell_allie @southendairport @easyjet probabl...</td>\n",
              "      <td>bell_allie southendairport easyjet probably to...</td>\n",
              "      <td>bell_allie southendairport easyjet probably fl...</td>\n",
              "      <td>bell_allie southendairport probably fly schedu...</td>\n",
              "      <td>bell_allie southendairport probably fly schedu...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01-Aug-2019 01:01AM</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>@RGrosjean @easyJet Go Pietro Go!!!____</td>\n",
              "      <td>@rgrosjean @easyjet go pietro go!!!____</td>\n",
              "      <td>rgrosjean easyjet go pietro go____</td>\n",
              "      <td>rgrosjean easyjet go pietro go____</td>\n",
              "      <td>rgrosjean go pietro go____</td>\n",
              "      <td>rgrosjean go pietro go____</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01-Aug-2019 01:01PM</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>@easyJet why does your submission form on your...</td>\n",
              "      <td>@easyjet why does your submission form on your...</td>\n",
              "      <td>easyjet why does your submission form on your ...</td>\n",
              "      <td>easyjet submission form website work sent emai...</td>\n",
              "      <td>submission form website work sent email receiv...</td>\n",
              "      <td>submission form website work sent email receiv...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01-Aug-2019 01:01PM</td>\n",
              "      <td>Italy</td>\n",
              "      <td>@Gatwick_Airport Hi, my flight to Pisa, with E...</td>\n",
              "      <td>@gatwick_airport hi, my flight to pisa, with e...</td>\n",
              "      <td>gatwick_airport hi my flight to pisa with easy...</td>\n",
              "      <td>gatwick_airport hi flight pisa easyjet delayed...</td>\n",
              "      <td>gatwick_airport hi pisa delayed 2 hours kind s...</td>\n",
              "      <td>gatwick_airport hi pisa delayed 2 hours kind s...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01-Aug-2019 01:02AM</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>=@brothers_beyond @easyJet Hi @brothers_beyond...</td>\n",
              "      <td>=@brothers_beyond @easyjet hi @brothers_beyond...</td>\n",
              "      <td>brothers_beyond easyjet hi brothers_beyond and...</td>\n",
              "      <td>brothers_beyond easyjet hi brothers_beyond eas...</td>\n",
              "      <td>brothers_beyond hi brothers_beyond ask ref lut...</td>\n",
              "      <td>brothers_beyond hi brothers_beyond ask ref lut...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Date         Country  \\\n",
              "0  01-Aug-2019 01:00AM  United Kingdom   \n",
              "1  01-Aug-2019 01:01AM         Unknown   \n",
              "2  01-Aug-2019 01:01PM         Unknown   \n",
              "3  01-Aug-2019 01:01PM           Italy   \n",
              "4  01-Aug-2019 01:02AM         Unknown   \n",
              "\n",
              "                                        Hit Sentence  \\\n",
              "0  =@bell_allie @SouthendAirport @easyJet Probabl...   \n",
              "1            @RGrosjean @easyJet Go Pietro Go!!!____   \n",
              "2  @easyJet why does your submission form on your...   \n",
              "3  @Gatwick_Airport Hi, my flight to Pisa, with E...   \n",
              "4  =@brothers_beyond @easyJet Hi @brothers_beyond...   \n",
              "\n",
              "                                          text_lower  \\\n",
              "0  =@bell_allie @southendairport @easyjet probabl...   \n",
              "1            @rgrosjean @easyjet go pietro go!!!____   \n",
              "2  @easyjet why does your submission form on your...   \n",
              "3  @gatwick_airport hi, my flight to pisa, with e...   \n",
              "4  =@brothers_beyond @easyjet hi @brothers_beyond...   \n",
              "\n",
              "                                          text_punct  \\\n",
              "0  bell_allie southendairport easyjet probably to...   \n",
              "1                 rgrosjean easyjet go pietro go____   \n",
              "2  easyjet why does your submission form on your ...   \n",
              "3  gatwick_airport hi my flight to pisa with easy...   \n",
              "4  brothers_beyond easyjet hi brothers_beyond and...   \n",
              "\n",
              "                                           text_stop  \\\n",
              "0  bell_allie southendairport easyjet probably fl...   \n",
              "1                 rgrosjean easyjet go pietro go____   \n",
              "2  easyjet submission form website work sent emai...   \n",
              "3  gatwick_airport hi flight pisa easyjet delayed...   \n",
              "4  brothers_beyond easyjet hi brothers_beyond eas...   \n",
              "\n",
              "                                         text_common  \\\n",
              "0  bell_allie southendairport probably fly schedu...   \n",
              "1                         rgrosjean go pietro go____   \n",
              "2  submission form website work sent email receiv...   \n",
              "3  gatwick_airport hi pisa delayed 2 hours kind s...   \n",
              "4  brothers_beyond hi brothers_beyond ask ref lut...   \n",
              "\n",
              "                                           text_rare text_urls  \n",
              "0  bell_allie southendairport probably fly schedu...      None  \n",
              "1                         rgrosjean go pietro go____      None  \n",
              "2  submission form website work sent email receiv...      None  \n",
              "3  gatwick_airport hi pisa delayed 2 hours kind s...      None  \n",
              "4  brothers_beyond hi brothers_beyond ask ref lut...      None  "
            ]
          },
          "execution_count": 24,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX8eCbeTz15x"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE924iM_z15x"
      },
      "outputs": [],
      "source": [
        "# Creating function for tokenization \n",
        "def tokenization(text):\n",
        "    text = re.split('\\W+', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xJa8skOz151",
        "outputId": "c5dcebba-f453-458b-95e6-1c7c8de7286e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[bell_allie, southendairport, probably, fly, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[rgrosjean, go, pietro, go____]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[submission, form, website, work, sent, email,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[gatwick_airport, hi, pisa, delayed, 2, hours,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[brothers_beyond, hi, brothers_beyond, ask, re...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          text_token\n",
              "0  [bell_allie, southendairport, probably, fly, s...\n",
              "1                    [rgrosjean, go, pietro, go____]\n",
              "2  [submission, form, website, work, sent, email,...\n",
              "3  [gatwick_airport, hi, pisa, delayed, 2, hours,...\n",
              "4  [brothers_beyond, hi, brothers_beyond, ask, re..."
            ]
          },
          "execution_count": 26,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Passing the function for tokenization \n",
        "df['text_token'] = df['text_rare'].apply(lambda x: tokenization(x.lower()))\n",
        "df[['text_token']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa9J7DTEz154"
      },
      "source": [
        "# Stemming and lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yo_BiP9z155"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfb5Mhphz158",
        "outputId": "5b73aaf5-7020-48c0-ac98-49469cc4a3a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/nikolajfrandsen/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/nikolajfrandsen/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ_5_jKbz16B"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX8KtFmCz16F"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV} \n",
        "# Pos tag, used Noun, Verb, Adjective, and Adverb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHXH2F08z16K"
      },
      "outputs": [],
      "source": [
        "#Function for lemmatization using POS\n",
        "def lemmatize_words(text):\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN))  for word, pos in pos_tagged_text])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9tut2Viz16O"
      },
      "outputs": [],
      "source": [
        "#Passing the function to 'text_rare' and store in 'text_lemma'\n",
        "df[\"text_lemma\"] = df[\"text_rare\"].apply(lemmatize_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qasHbErDz16R",
        "outputId": "c17fc0b1-8cff-4fd2-89e3-3139710c7e8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    bell_allie southendairport probably fly schedu...\n",
              "1                           rgrosjean go pietro go____\n",
              "2    submission form website work send email receiv...\n",
              "3    gatwick_airport hi pisa delay 2 hour kind seri...\n",
              "4    brothers_beyond hi brothers_beyond ask ref lut...\n",
              "Name: text_lemma, dtype: object"
            ]
          },
          "execution_count": 33,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"text_lemma\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87jmQHZ5z16U",
        "outputId": "07524b9d-3ac3-469c-a1a1-44a4e7c64d39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Date            89544\n",
              "Country         89544\n",
              "Hit Sentence    89544\n",
              "text_lower      89544\n",
              "text_punct      89544\n",
              "text_stop       89544\n",
              "text_common     89544\n",
              "text_rare       89544\n",
              "text_urls           0\n",
              "text_token      89544\n",
              "text_lemma      89544\n",
              "dtype: int64"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4O3ENi3z16W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cCaK9yrz16b"
      },
      "outputs": [],
      "source": [
        "df.to_csv('/Users/nikolajfrandsen/Desktop/EasyJet_PreProcessed_ORIGINAL.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzHwSudmz16g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0CQb_Wiz16j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}